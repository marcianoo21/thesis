# ğŸ¤– Konwersacyjny System Rekomendacji Restauracji â€” ÅÃ³dÅº

PeÅ‚noprawny system RAG (Retrieval-Augmented Generation) do rekomendacji restauracji i kawiarni w Åodzi z moÅ¼liwoÅ›ciÄ… prowadzenia naturalnej konwersacji.

## ğŸ¯ Komponenty

### 1. **`conversational_rag.py`** â€” Silnik RAG

- `ConversationHistory` â€” zarzÄ…dza historiÄ… czatu
- `ConversationalRAG` â€” gÅ‚Ã³wna klasa integrujÄ…ca LLM + RAG
- **Adaptery LLM:**
  - `OpenAILLM` â€” OpenAI API (GPT-3.5/GPT-4)
  - `OllamaLLM` â€” lokalny Ollama (offline)
  - `SimpleLLM` â€” tryb demo (bez API)

### 2. **`chat_interface.py`** â€” Interaktywny interfejs

- GÅ‚Ã³wna pÄ™tla czatu
- Integracja z embeddingami FAISS
- Export konwersacji do JSON

### 3. **`example_rag_usage.py`** â€” PrzykÅ‚ady uÅ¼ycia

- Demo z SimpleLLM
- Demo z OpenAI API
- Niestandardowe system prompty

---

## ğŸš€ Szybki Start

### Opcja 1: SimpleLLM (brak API, tryb demo)

```bash
python chat_interface.py
```

**WyjÅ›cie:**

```
ğŸ¤– KONWERSACYJNY SYSTEM REKOMENDACJI RESTAURACJI â€” ÅÃ“DÅ¹

ğŸ‘¤ Ty: Szukam dobrej kawiarni blisko centrum
ğŸ¤– Asystent: Na podstawie Twojego zapytania znalazÅ‚em kilka Å›wietnych opcji:

Znalezione restauracje:
1. **The Brick Coffee Factory**
   Typ: kawa
   Adres: ul. Piotrkowska 123
   Ocena: 4.6â­ (314 opinii)
   Dopasowanie: 0.856

...
```

### Opcja 2: OpenAI API (wymaga API key)

**1. Ustaw zmienne Å›rodowiskowe w `.env`:**

```env
OPENAI_API_KEY=sk-proj-xxxxx
```

**2. Uruchom czat:**

```bash
python chat_interface.py
```

**3. (Opcjonalnie) Testuj przykÅ‚ady:**

```bash
python example_rag_usage.py
```

---

## ğŸ’¬ Jak to dziaÅ‚a?

### Flow Konwersacji

```
UÅ¼ytkownik: "Szukam sushi dla 4 osÃ³b"
    â†“
[chat_interface.py] â†’ wczytuje input
    â†“
[ConversationalRAG.generate_response()]
    â”œâ”€ 1. Dodaj do historii
    â”œâ”€ 2. Wyszukaj RAG: search_restaurants("szukam sushi...")
    â”œâ”€ 3. Pobierz kontekst: restauracje + oceny + adresy
    â”œâ”€ 4. Konstruuj prompt:
    â”‚    System: "JesteÅ› asystentem rekomendacji..."
    â”‚    Kontekst: "Znalezione: Sphinx (4.7â­), Hana Sushi (4.1â­)..."
    â”‚    Historia: "Poprzednie pytania..."
    â””â”€ 5. WyÅ›lij do LLM â†’ Otrzymaj odpowiedÅº
    â†“
Asystent: "Åšwietnie! Mam idealne opcje dla Was:
1. Sphinx - 4.7â­ (10616 opinii) - najwyÅ¼ej oceniany
2. Hana Sushi - 4.1â­ (1035 opinii) - bardziej kameralne
Czy ktÃ³raÅ› z nich Wam pasuje?"
    â†“
[Dodaj odpowiedÅº do historii] â†’ Gotowe na nastÄ™pne pytanie
```

---

## âš™ï¸ Konfiguracja

### System Prompt

DomyÅ›lny system prompt znajduje siÄ™ w `ConversationalRAG._default_system_prompt()`.

Aby zmieniÄ‡, uÅ¼yj:

```python
custom_prompt = "JesteÅ› asystentem specjalizujÄ…cym siÄ™ w gastronomii..."

rag = ConversationalRAG(
    llm_client=llm,
    search_function=search,
    system_prompt=custom_prompt,
)
```

### Model LLM

**OpenAI:**

```python
from conversational_rag import OpenAILLM

llm = OpenAILLM(
    api_key="sk-...",
    model="gpt-4"  # lub "gpt-3.5-turbo"
)
```

**Ollama (offline):**

```python
from conversational_rag import OllamaLLM

llm = OllamaLLM(
    model="mistral",  # lub "llama2", "neural-chat"
    base_url="http://localhost:11434"
)
```

**SimpleLLM (demo):**

```python
from conversational_rag import SimpleLLM

llm = SimpleLLM()  # Zwraca template, nie wymaga API
```

---

## ğŸ“Š Struktura Danych

### Historia Konwersacji

```json
{
  "messages": [
    {
      "role": "user",
      "content": "Szukam sushi",
      "timestamp": "2025-11-30T10:30:45.123456"
    },
    {
      "role": "assistant",
      "content": "ZnalazÅ‚em kilka opcji...",
      "timestamp": "2025-11-30T10:30:46.234567"
    }
  ]
}
```

### Wyniki RAG

```json
{
  "name": "Sphinx",
  "type": "restauracja",
  "address": "ul. Piotrkowska 100",
  "rating": 4.7,
  "reviews": 10616,
  "relevance_score": 0.856
}
```

---

## ğŸ”§ Zaawansowane UÅ¼ycie

### Programmatyczne Wyzwolenie

```python
from conversational_rag import ConversationalRAG, SimpleLLM

# ZaÅ‚aduj komponenty
search = load_search_engine()
llm = SimpleLLM()

# StwÃ³rz RAG
rag = ConversationalRAG(
    llm_client=llm,
    search_function=search,
    max_history=10,  # Ostatnie 10 wiadomoÅ›ci
)

# ProwadÅº konwersacjÄ™
response = rag.generate_response("ChcÄ™ pizzÄ™")
print(response)

# Eksportuj historiÄ™
rag.export_conversation("my_chat.json")
```

### DostÄ™p do Historii

```python
# Pobierz wszystkie wiadomoÅ›ci
history = rag.get_history()

# Uzyskaj kontekst dla innego modelu
context = rag.history.get_context()

# Pobierz w formacie OpenAI API
messages = rag.history.get_messages_for_api()
```

---

## ğŸ“‹ Wymagania

```
sentence-transformers>=2.2.0
faiss-cpu>=1.7.0  # lub faiss-gpu
numpy>=1.21.0
openai>=1.0.0  # Tylko jeÅ›li chcesz OpenAI API
geopy>=2.2.0
python-dotenv>=0.19.0
```

**Instalacja:**

```bash
pip install sentence-transformers faiss-cpu numpy openai geopy python-dotenv
```

---

## ğŸ“ PrzykÅ‚ady PromptÃ³w do Testowania

SprÃ³buj tych pytaÅ„:

1. **Wyszukiwanie typu kuchni:**

   - "Gdzie mogÄ™ zjeÅ›Ä‡ sushi w Åodzi?"
   - "Szukam dobrej wÅ‚oskiej restauracji"

2. **Wyszukiwanie z kontekstem:**

   - "Restauracja na romantycznÄ… kolacjÄ™ dla dwojga"
   - "Gdzie mogÄ™ piÄ‡ piwo z przyjaciÃ³Å‚mi?"

3. **Wyszukiwanie z ograniczeniami:**

   - "Dobra pizza poniÅ¼ej 40 zÅ‚ za osobÄ™"
   - "Kawiarnia do pracy ze spokojnÄ… atmosferÄ…"

4. **Follow-up pytania:**
   - "Czy to blisko centrum?"
   - "Ile to kosztuje?"
   - "Jak mogÄ™ tam dojechaÄ‡?"

---

## ğŸ› Troubleshooting

### BÅ‚Ä…d: "ModuleNotFoundError: No module named 'openai'"

```bash
pip install openai
```

### BÅ‚Ä…d: "FAISS index is empty"

Upewnij siÄ™, Å¼e `lodz_restaurants_cafes_embeddings.jsonl` istnieje i zawiera dane.

### BÅ‚Ä…d: "API rate limit exceeded"

OpenAI ma limity. Poczekaj chwilÄ™ lub uÅ¼yj `SimpleLLM`.

### Odpowiedzi sÄ… generyczne

SprawdÅº, czy RAG znajduje restauracje:

```python
results = rag.search_restaurants_rag("twoje pytanie", k=5)
print(results)
```

---

## ğŸ“ Licencja

MIT

---

## ğŸ¤ Kontakt

Pytania? ZgÅ‚aszaj na GitHub! ğŸš€
