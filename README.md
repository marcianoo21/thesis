## ğŸ“ Konwersacyjny System Rekomendacji Restauracji w Åodzi â€” README (praca dyplomowa)

Ten projekt jest kompletnym systemem **RAG (Retrieval-Augmented Generation)** do rekomendacji restauracji, kawiarni i innych lokali gastronomicznych w **Åodzi**.  
System Å‚Ä…czy:
- **PLLuM-12B** (model jÄ™zykowy, Hugging Face Inference API) â€“ naturalna konwersacja po polsku,
- **wÅ‚asne embeddingi** (RoBERTa / STELLA) przechowywane w plikach `*.jsonl`,
- **FAISS** â€“ szybkie wektorowe wyszukiwanie,
- **warstwÄ™ lokalizacji** (spaCy + Nominatim) â€“ rozumienie potocznych nazw miejsc i dzielnic w Åodzi,
- **logikÄ™ rankingowÄ…** â€“ Å‚Ä…czenie podobieÅ„stwa semantycznego z ocenami, popularnoÅ›ciÄ… i odlegÅ‚oÅ›ciÄ…,
- **interfejs tekstowy oraz prosty frontend webowy (Flask + HTML)**.

Celem README jest umoÅ¼liwienie **osobie z zewnÄ…trz**:
- zainstalowania Å›rodowiska,
- zainicjowania danych / embeddingÃ³w,
- uruchomienia systemu w trybie linii komend i przez przeglÄ…darkÄ™,
- uruchomienia skryptÃ³w testowych i ewaluacyjnych.

---

## ğŸ“¦ Struktura projektu (wysoki poziom)

NajwaÅ¼niejsze katalogi i pliki:

```text
.
â”œâ”€â”€ app.py                     # Backend Flask (API do interfejsu webowego)
â”œâ”€â”€ chat_ui.html               # Frontend webowy (czat w przeglÄ…darce)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py            # Ujawnia API pakietu src (create_rag_system, LocationService, ModelMeanPooling, itp.)
â”‚   â”œâ”€â”€ conversational_rag.py  # GÅ‚Ã³wny silnik RAG (PLLuM + FAISS + ranking)
â”‚   â”œâ”€â”€ config.py              # Profile konfiguracyjne (style odpowiedzi, top_k, itp.)
â”‚   â”œâ”€â”€ embedding_model.py     # Wrapper na SentenceTransformers (ModelMeanPooling)
â”‚   â””â”€â”€ location_service.py    # Warstwa lokalizacji (spaCy + Nominatim)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py        # Konsolowy pipeline rekomendacji (1 zapytanie â†’ lista miejsc)
â”‚   â”œâ”€â”€ chat_interface.py      # Interaktywny czat w terminalu (konwersacyjny asystent)
â”‚   â”œâ”€â”€ data_gathering.py      # (opcjonalnie) pobieranie / przygotowanie danych ÅºrÃ³dÅ‚owych
â”‚   â”œâ”€â”€ extract_keywords.py    # ekstrakcja sÅ‚Ã³w kluczowych
â”‚   â”œâ”€â”€ key_words_and_context_creation.py
â”‚   â”œâ”€â”€ context_creation_only_words.py
â”‚   â”œâ”€â”€ chunk_divide.py        # dzielenie dÅ‚uÅ¼szych opisÃ³w na fragmenty
â”‚   â”œâ”€â”€ search_restaurants.py  # narzÄ™dzia wyszukiwawcze
â”‚   â””â”€â”€ ...                    # inne skrypty analityczne i pomocnicze
â”‚
â”œâ”€â”€ embedding_creation/
â”‚   â”œâ”€â”€ create_embeddings_mean.py
â”‚   â”œâ”€â”€ create_embeddings_cls.py
â”‚   â”œâ”€â”€ create_embeddings_mean_words.py
â”‚   â”œâ”€â”€ create_embeddings_cls_words.py
â”‚   â”œâ”€â”€ create_embeddings_mean_stella.py
â”‚   â”œâ”€â”€ create_embeddings_cls_stella.py
â”‚   â”œâ”€â”€ create_embeddings_mean_words_stella.py
â”‚   â”œâ”€â”€ create_embeddings_cls_words_stella.py
â”‚   â”œâ”€â”€ create_embeddings_cls_words_v2.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ output_files/              # Dane wejÅ›ciowe / poÅ›rednie / embeddingi
â”‚   â”œâ”€â”€ lodz_restaurants_cafes.csv
â”‚   â”œâ”€â”€ lodz_restaurants_cafes_with_ratings*.jsonl
â”‚   â”œâ”€â”€ lodz_restaurants_cafes_with_key_words*.jsonl
â”‚   â”œâ”€â”€ lodz_restaurants_cafes_chunks.jsonl
â”‚   â”œâ”€â”€ lodz_restaurants_cafes_emb_input*.jsonl
â”‚   â”œâ”€â”€ context_from_filtered_keywords.jsonl
â”‚   â”œâ”€â”€ lodz_restaurants_cafes_embeddings_*.jsonl   # RÃ³Å¼ne warianty embeddingÃ³w
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_location_layer.py               # test warstwy lokalizacji
â”‚   â”œâ”€â”€ run_embedding_tests.py               # porÃ³wnanie rÃ³Å¼nych wariantÃ³w embeddingÃ³w
â”‚   â”œâ”€â”€ run_embedding_tests_v2_comparison.py # porÃ³wnanie modelu v1 vs v2
â”‚   â”œâ”€â”€ run_embedding_tests_stella.py        # testy dla modeli STELLA
â”‚   â”œâ”€â”€ run_query_expansion_tests.py         # test HyDE / query expansion
â”‚   â”œâ”€â”€ run_location_tests.py                # test normalizacji lokalizacji (LLM + spaCy)
â”‚   â””â”€â”€ evaluate_full_pipeline.py            # ewaluacja peÅ‚nego pipeline'u (MRR, hit rate)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md                # (ten plik)
```

---

## âœ… Wymagania wstÄ™pne

- **Python**: rekomendowana wersja 3.10+  
- **System**: Linux / macOS / Windows (testowane na Windows 10/11)
- **Konto Hugging Face** z waÅ¼nym tokenem API (uÅ¼ywany przez `PLLuMLLM`):
  - model: `CYFRAGOVPL/PLLuM-12B-nc-chat`
- DostÄ™p do Internetu (PLLuM dziaÅ‚a przez inference API + Nominatim dla geokodowania).
- Wymagane pakiety Python (instalowane przez `requirements.txt`), m.in.:
  - `huggingface_hub`
  - `sentence-transformers`
  - `faiss-cpu`
  - `spacy`
  - `geopy`
  - `python-dotenv`
  - `flask`, `flask-cors`
  - `scipy`, `numpy`, `pandas`, itp.

Dodatkowo:
- Model spaCy: **`pl_core_news_lg`** (do wykrywania nazw lokalizacji).

---

## ğŸ”§ Instalacja krok po kroku

### 1. Sklonuj repozytorium

W terminalu / PowerShell:

```bash
git clone <URL_twojego_repozytorium>
cd InÅ¼ynierka    # lub inna nazwa katalogu z projektem
```

### 2. UtwÃ³rz i aktywuj wirtualne Å›rodowisko (zalecane)

PrzykÅ‚ad (Windows, `venv`):

```bash
python -m venv venv
venv\Scripts\activate
```

Linux / macOS:

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Zainstaluj zaleÅ¼noÅ›ci

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

JeÅ›li pojawiÄ… siÄ™ problemy z FAISS, moÅ¼esz jawnie doinstalowaÄ‡:

```bash
pip install faiss-cpu
```

### 4. Zainstaluj model spaCy `pl_core_news_lg`

```bash
python -m spacy download pl_core_news_lg
```

### 5. Konfiguracja tokenu Hugging Face (`HF_TOKEN`)

1. ZaÅ‚Ã³Å¼ konto na `https://huggingface.co/` (jeÅ›li jeszcze nie masz).
2. WejdÅº w ustawienia tokenÃ³w: `https://huggingface.co/settings/tokens`.
3. UtwÃ³rz token z uprawnieniami **read**.
4. W katalogu projektu utwÃ³rz plik `.env` (jeÅ›li go nie ma) i wpisz:

```env
HF_TOKEN=hf_...tutaj_twÃ³j_token...
```

Plik `.env` **nie powinien byÄ‡ commitowany** do repozytorium (jest w `.gitignore`).

---

## ğŸ§± Dane i embeddingi â€“ jak to jest zorganizowane

W folderze `output_files/` znajdujÄ… siÄ™ kolejne etapy przetwarzania danych:

- **Dane ÅºrÃ³dÅ‚owe**:  
  - `lodz_restaurants_cafes.csv` â€“ surowa tabela restauracji/kawiarni.

- **Wzbogacanie danych** (oceny, sÅ‚owa kluczowe, opisy):
  - `lodz_restaurants_cafes_with_ratings*.jsonl`
  - `lodz_restaurants_cafes_with_key_words*.jsonl`
  - `lodz_restaurants_cafes_chunks.jsonl` â€“ podziaÅ‚ dÅ‚uÅ¼szych opisÃ³w na fragmenty.

- **Przygotowanie tekstÃ³w do embeddingÃ³w**:
  - `lodz_restaurants_cafes_emb_input*.jsonl` â€“ teksty kontekstowe (peÅ‚ne opisy) dla embeddingÃ³w,
  - `context_from_filtered_keywords.jsonl` â€“ teksty zbudowane tylko z wybranych sÅ‚Ã³w kluczowych.

- **Gotowe embeddingi** (rÃ³Å¼ne warianty modelu i poolingu):
  - `lodz_restaurants_cafes_embeddings_mean.jsonl`
  - `lodz_restaurants_cafes_embeddings_cls.jsonl`
  - `lodz_restaurants_cafes_embeddings_mean_words.jsonl`
  - `lodz_restaurants_cafes_embeddings_cls_words.jsonl`
  - `lodz_restaurants_cafes_embeddings_mean_stella.jsonl`
  - `lodz_restaurants_cafes_embeddings_cls_stella.jsonl`
  - `lodz_restaurants_cafes_embeddings_mean_words_stella.jsonl`
  - `lodz_restaurants_cafes_embeddings_cls_words_stella.jsonl`
  - `lodz_restaurants_cafes_embeddings_cls_words_v2.jsonl`
  - itp.

W praktyce **do dziaÅ‚ania systemu nie musisz generowaÄ‡ embeddingÃ³w od zera** â€“ w repozytorium sÄ… juÅ¼ gotowe pliki.  
DomyÅ›lnie backend (`app.py`) korzysta z:

- `output_files/lodz_restaurants_cafes_embeddings_cls_words.jsonl`

JeÅ›li chcesz samodzielnie odtworzyÄ‡ pipeline (np. do celÃ³w naukowych), zobacz nastÄ™pny punkt.

---

## ğŸ”¬ (Opcjonalnie) PeÅ‚ny pipeline tworzenia embeddingÃ³w

Ten krok jest **opcjonalny** â€“ potrzebny tylko wtedy, gdy chcesz:
- zbudowaÄ‡ embeddingi z nowego ÅºrÃ³dÅ‚a danych,
- przetestowaÄ‡ inne warianty (np. inny model, inny pooling).

### 1. Przygotowanie danych wejÅ›ciowych

Å¹rÃ³dÅ‚owym plikiem jest zwykle `lodz_restaurants_cafes.csv` (kolumny z nazwÄ…, opisem, adresem, itp.).  
NastÄ™pnie skrypty w `scripts/` wykonujÄ… m.in.:

- ekstrakcjÄ™ sÅ‚Ã³w kluczowych (`extract_keywords.py`, `key_words_and_context_creation.py`),
- budowÄ™ kontekstÃ³w (`context_creation_only_words.py`),
- dzielenie tekstÃ³w na fragmenty (`chunk_divide.py`).

W rezultacie powstajÄ… pliki `*_with_key_words*.jsonl` oraz `context_from_filtered_keywords.jsonl` / `lodz_restaurants_cafes_emb_input*.jsonl`.

### 2. Generowanie embeddingÃ³w â€“ przykÅ‚ady

W katalogu `embedding_creation/` masz zestaw skryptÃ³w. Uruchamiasz je **z katalogu gÅ‚Ã³wnego projektu**, np.:

```bash
python -m embedding_creation.create_embeddings_mean
python -m embedding_creation.create_embeddings_cls
python -m embedding_creation.create_embeddings_mean_words
python -m embedding_creation.create_embeddings_cls_words
```

OdpowiadajÄ… one za:
- **`create_embeddings_mean.py`** â€“ peÅ‚ne konteksty, pooling *mean*,
- **`create_embeddings_cls.py`** â€“ peÅ‚ne konteksty, pooling *CLS*,
- **`create_embeddings_mean_words.py`** â€“ tylko sÅ‚owa kluczowe, pooling *mean*,
- **`create_embeddings_cls_words.py`** â€“ tylko sÅ‚owa kluczowe, pooling *CLS*.

Analogicznie, warianty `*_stella.py` oraz `*_v2.py` korzystajÄ… z modelu **STELLA** lub nowszej wersji modelu embeddingowego.

KaÅ¼dy skrypt:
- wczytuje metadane z `lodz_restaurants_cafes_with_key_words.jsonl`,
- wczytuje tekst do zakodowania (`context`),
- wywoÅ‚uje `ModelMeanPooling` z `src/embedding_model.py`,
- zapisuje wynik do `output_files/lodz_restaurants_cafes_embeddings_*.jsonl`.

---

## ğŸš€ Uruchomienie systemu â€“ tryb webowy (Flask + przeglÄ…darka)

To jest **najwaÅ¼niejszy scenariusz uÅ¼ytkowy** â€“ uruchomienie asystenta rekomendacji w przeglÄ…darce.

### 1. Upewnij siÄ™, Å¼e masz:

- aktywne Å›rodowisko wirtualne,
- zainstalowane zaleÅ¼noÅ›ci (`pip install -r requirements.txt`),
- zainstalowany model spaCy: `pl_core_news_lg`,
- plik `.env` z poprawnym `HF_TOKEN`,
- istnieje plik embeddingÃ³w wskazywany w `app.py` (domyÅ›lnie):

```python
embedding_file = "output_files/lodz_restaurants_cafes_embeddings_cls_words.jsonl"
```

### 2. Uruchom backend Flask

Z katalogu gÅ‚Ã³wnego projektu:

```bash
python app.py
```

Co siÄ™ dzieje w `app.py`:
- wczytywany jest `.env`,
- tworzony jest `LocationService` (spaCy + Nominatim),
- wywoÅ‚ywana jest funkcja `create_rag_system(...)` z `src/conversational_rag.py`,
- budowany jest globalny obiekt `rag_chain` (silnik RAG),
- startuje serwer Flask (domyÅ›lnie na porcie `5000`).

JeÅ›li wszystko jest poprawnie skonfigurowane, w konsoli zobaczysz komunikaty typu:

```text
--- Inicjalizacja serwera backendu ---
Åadowanie modelu i embeddingÃ³w...
...
--- System gotowy do pracy ---
 * Running on http://127.0.0.1:5000
```

### 3. OtwÃ³rz interfejs webowy

Plik `chat_ui.html` jest statycznym frontendem, ktÃ³ry Å‚Ä…czy siÄ™ z API `app.py`.  
MoÅ¼esz go otworzyÄ‡ na dwa sposoby:

- **BezpoÅ›rednio z dysku** (double-click / â€OtwÃ³rz w przeglÄ…darceâ€):
  - strona bÄ™dzie wysyÅ‚aÄ‡ Å¼Ä…dania `POST` na `http://localhost:5000/chat`.

- Lub hostujÄ…c go przez serwer (np. inny prosty backend) â€“ ale w tym projekcie wystarcza zwykÅ‚e otwarcie pliku .html.

Po otwarciu zobaczysz pole czatu, suwak ceny, itp.  
Backend endpointy w `app.py`:
- `GET /` â€“ serwuje `chat_ui.html` (jeÅ›li otwierasz przez Flask),
- `POST /chat` â€“ przyjmuje JSON `{ message: "...", price_level: 0..3 }` i zwraca HTML z listÄ… rekomendacji.

---

## ğŸ’¬ Uruchomienie systemu â€“ tryb czat w terminalu

JeÅ›li wolisz interfejs konsolowy, uÅ¼yj `scripts/chat_interface.py`.

### 1. Uruchom

Z katalogu gÅ‚Ã³wnego:

```bash
python scripts/chat_interface.py --profile default \
  --embedding-file output_files/lodz_restaurants_cafes_embeddings_mean.jsonl
```

Parametry:
- `--profile` â€“ wybÃ³r profilu z `src/config.py` (`default`, `fast`, `detailed`, `friendly`, `professional`, `local`, `budget`, `foodie`),
- `--embedding-file` â€“ Å›cieÅ¼ka do pliku z embeddingami.

### 2. Komendy specjalne w czacie

W trakcie rozmowy moÅ¼esz wpisaÄ‡:
- `exit`, `quit`, `q` â€“ zakoÅ„czenie programu,
- `clear`, `reset` â€“ wyczyszczenie historii konwersacji,
- `save` / `zapisz` â€“ zapisanie historii konwersacji do pliku JSON,
- `profile` / `profil` â€“ wyÅ›wietlenie aktualnego profilu konfiguracyjnego.

PrzykÅ‚ad dialogu:

```text
Ty: Dobra pizza w centrum
Asystent: (lista kilku pizzerii z adresami, ocenÄ…, odlegÅ‚oÅ›ciÄ…, itp.)

Ty: A coÅ› romantycznego na randkÄ™?
Asystent: (kolejna lista, pamiÄ™tajÄ…ca wczeÅ›niejsze preferencje)
```

---

## ğŸ” Jednorazowy pipeline rekomendacji (skrypt `run_pipeline.py`)

JeÅ›li chcesz jednorazowo przetestowaÄ‡ pipeline (bez konwersacji, ale z wejÅ›ciami z klawiatury), uÅ¼yj:

```bash
python scripts/run_pipeline.py \
  --embedding-file output_files/lodz_restaurants_cafes_embeddings_cls_words.jsonl \
  -k 5
```

Skrypt:
- poprosi CiÄ™ o tekst zapytania (np. â€tania pizza na widzewieâ€),
- sprÃ³buje wywnioskowaÄ‡ lokalizacjÄ™ (LLM + spaCy),
- ewentualnie dopyta o lokalizacjÄ™ i przedziaÅ‚ cenowy,
- wypisze listÄ™ rekomendacji w terminalu (z ocenami, odlegÅ‚oÅ›ciÄ…, kontekstem).

---

## ğŸ§ª Testy i ewaluacja

W katalogu `tests/` znajdujÄ… siÄ™ skrypty analizujÄ…ce rÃ³Å¼ne aspekty systemu.

### 1. Test warstwy lokalizacji

```bash
python tests/test_location_layer.py
```

Skrypt:
- uruchamia LLM (`PLLuMLLM`) i `LocationService`,
- przechodzi przez listÄ™ testowych zapytaÅ„ z Å‚Ã³dzkim slangiem (np. â€kawa koÅ‚o manuâ€, â€kebab na gÃ³rniakuâ€),
- sprawdza, czy system poprawnie normalizuje lokalizacje i znajduje wspÃ³Å‚rzÄ™dne,
- zapisuje wyniki do `location_test_results.csv`.

### 2. Ewaluacja embeddingÃ³w (rÃ³Å¼ne warianty)

```bash
python tests/run_embedding_tests.py
```

Skrypt:
- testuje kilka plikÃ³w embeddingÃ³w (`mean`, `cls`, `*_words`, itp.),
- dla predefiniowanego zestawu zapytaÅ„ (`GROUND_TRUTH`) oblicza:
  - Hit Rate@5,
  - MRR,
  - Precision@5,
- zapisuje wyniki do:
  - `embedding_test_results_all_retrieval.txt`,
  - `embedding_metrics_summary_retrieval.csv`.

Analogiczne skrypty:
- `tests/run_embedding_tests_stella.py` â€“ warianty z modelami STELLA,
- `tests/run_embedding_tests_v2_comparison.py` â€“ porÃ³wnanie roberta v1 vs v2.

### 3. Test query expansion (HyDE)

```bash
python tests/run_query_expansion_tests.py
```

Skrypt porÃ³wnuje wyniki wyszukiwania dla:
- surowego zapytania uÅ¼ytkownika,
- zapytania rozszerzonego przez LLM (HyDE) na opisy â€idealnychâ€ restauracji.

Wyniki zapisuje do `query_expansion_comparison_results.txt`.

### 4. Test detekcji lokalizacji (LLM + spaCy)

```bash
python tests/run_location_tests.py
```

Testowane jest dziaÅ‚anie metody `normalize_location` w `ConversationalRAG` oraz `LocationService`.

### 5. Ewaluacja peÅ‚nego pipeline'u

```bash
python tests/evaluate_full_pipeline.py
```

Skrypt:
- pobiera kandydatÃ³w z FAISS,
- stosuje reranker,
- oblicza metryki dla kolejnych etapÃ³w (`bi-encoder`, `reranker`, koÅ„cowy algorytm wagowy),
- wypisuje tabelÄ™ z wynikami (Hit Rate@5, MRR, Precision@5).

---

## âš™ï¸ GÅ‚Ã³wne komponenty kodu (skrÃ³t)

- **`src/conversational_rag.py`**
  - `PLLuMLLM` â€“ klient Hugging Face Inference API dla PLLuM-12B,
  - `ConversationalRAG` â€“ gÅ‚Ã³wna klasa systemu:
    - `analyze_user_intent` â€“ jedna rozmowa z LLM, ktÃ³ra wyciÄ…ga intencjÄ™, lokalizacjÄ™, typ kuchni, cenÄ™,
    - `extract_search_query` â€“ HyDE / query expansion,
    - `normalize_location`, `extract_cuisine_type`, `normalize_price` â€“ pomocnicze ekstraktory,
    - `generate_response` â€“ Å‚Ä…czy wyszukiwanie wektorowe, reranking i generowanie odpowiedzi,
    - `_format_search_results` â€“ zamienia listÄ™ wynikÃ³w na HTML (uÅ¼ywane w `app.py`),
    - `_is_open_now` â€“ heurystyczne sprawdzanie, czy lokal jest aktualnie otwarty.
  - `create_rag_system(...)` â€“ gÅ‚Ã³wna fabryka systemu:
    - Å‚aduje embeddingi z pliku,
    - tworzy indeks FAISS,
    - Å‚aduje model `ModelMeanPooling`,
    - Å‚aduje reranker (`sdadas/polish-reranker-roberta-v2`),
    - skÅ‚ada to w gotowy `ConversationalRAG` + funkcjÄ™ `search(...)`.

- **`src/config.py`**
  - zawiera klasy i profile konfiguracyjne (`RAGConfig` + `PROFILES`),
  - kaÅ¼dy profil ma zdefiniowany system prompt (z mocnym ograniczeniem: tylko gastronomia w Åodzi).

- **`src/location_service.py`**
  - `LocationService` â€“ ekstrakcja nazw lokalizacji (spaCy) + geokodowanie (Nominatim),
  - potrafi radziÄ‡ sobie z faÅ‚szywymi pozytywami (np. â€wÅ‚oskaâ€ jako kuchnia, a nie lokalizacja).

- **`src/embedding_model.py`**
  - `ModelMeanPooling` â€“ wrapper na SentenceTransformers z wyborem strategii: `mean` lub `cls`.

---

## ğŸ› Typowe problemy i rozwiÄ…zania

- **BÅ‚Ä…d: â€Brak HF_TOKENâ€ lub â€HF_TOKEN not setâ€**
  - SprawdÅº plik `.env` w katalogu projektu,
  - Upewnij siÄ™, Å¼e jest tam linia `HF_TOKEN=...`,
  - MoÅ¼esz teÅ¼ ustawiÄ‡ zmiennÄ… Å›rodowiskowÄ… systemowo.

- **BÅ‚Ä…d: â€OSError: [E050] Can't find model 'pl_core_news_lg'â€**
  - Uruchom:
    ```bash
    python -m spacy download pl_core_news_lg
    ```

- **BÅ‚Ä…d: â€FileNotFoundError: ...embeddings_*.jsonlâ€**
  - SprawdÅº, czy wskazany w `app.py` / `scripts/*` plik istnieje w `output_files/`,
  - JeÅ›li nie istnieje, wygeneruj go wybranym skryptem z `embedding_creation/`.

- **Backend Flask siÄ™ uruchamia, ale frontend nie widzi odpowiedzi**
  - Upewnij siÄ™, Å¼e:
    - backend dziaÅ‚a na `http://localhost:5000`,
    - przeglÄ…darka nie blokuje Å¼Ä…daÅ„ CORS (w `app.py` jest `flask_cors.CORS`).

- **Odpowiedzi nie sÄ… zwiÄ…zane z gastronomiÄ… lub mÃ³wiÄ… o podrÃ³Å¼ach / innych sprawach**
  - W aktualnej wersji prompt systemowy jest ograniczony **wyÅ‚Ä…cznie** do gastronomii w Åodzi,
  - jeÅ›li model mimo to â€uciekaâ€ w inne tematy, to jest to wÅ‚aÅ›ciwoÅ›Ä‡ modelu PLLuM; w pracy dyplomowej opisuj to jako ograniczenie LLM.

---

## ğŸ“„ Licencja

Projekt jest udostÄ™pniany na licencji **MIT** (patrz plik `LICENSE`).  

---

## ğŸ“š Kontekst pracy dyplomowej

Ten projekt stanowi czÄ™Å›Ä‡ pracy inÅ¼ynierskiej poÅ›wiÄ™conej:
- budowie konwersacyjnego systemu rekomendacji gastronomicznych w mieÅ›cie (ÅÃ³dÅº),
- porÃ³wnaniu rÃ³Å¼nych strategii embeddingÃ³w i modeli,
- analizie wpÅ‚ywu dodatkowych czynnikÃ³w (oceny, liczba opinii, odlegÅ‚oÅ›Ä‡) na ranking,
- testowaniu warstwy lokalizacji i query expansion.
