# ğŸ¤– Konwersacyjny System Rekomendacji Restauracji

System do rekomendacji restauracji w Åodzi wykorzystujÄ…cy:
- **PLLuM-12B** - polski model jÄ™zykowy do naturalnej konwersacji
- **RoBERTa embeddings** - semantyczne wyszukiwanie
- **FAISS** - szybkie wyszukiwanie wektorowe
- **RAG** - Retrieval Augmented Generation

## ğŸš€ Instalacja

### 1. Sklonuj repozytorium
```bash
git clone <your-repo>
cd restaurant-recommender
```

### 2. Zainstaluj zaleÅ¼noÅ›ci
```bash
pip install -r requirements.txt
```

### 3. Konfiguracja API Token

Potrzebujesz tokenu Hugging Face:
1. Zarejestruj siÄ™ na https://huggingface.co/
2. PrzejdÅº do https://huggingface.co/settings/tokens
3. UtwÃ³rz nowy token (Read access wystarczy)

Skopiuj `.env.example` do `.env` i wklej swÃ³j token:
```bash
cp .env.example .env
nano .env  # lub edytor tekstu
```

Wpisz:
```
HF_TOKEN=hf_twÃ³j_token_tutaj
```

### 4. Przygotuj embeddingi

JeÅ›li jeszcze nie masz pliku z embeddingami:
```bash
python create_embeddings_mean.py
# lub
python create_embeddings_cls.py
```

## ğŸ“ Struktura projektu

```
.
â”œâ”€â”€ conversational_rag.py          # GÅ‚Ã³wny system RAG
â”œâ”€â”€ chat_interface.py              # Interfejs czatu
â”œâ”€â”€ test_system.py                 # Skrypty testowe
â”œâ”€â”€ create_embeddings_mean.py      # Tworzenie embeddingÃ³w (mean pooling)
â”œâ”€â”€ create_embeddings_cls.py       # Tworzenie embeddingÃ³w (CLS pooling)
â”œâ”€â”€ embedding_model.py             # Model embeddingÃ³w
â”œâ”€â”€ search_restaurants.py          # Podstawowe wyszukiwanie
â”œâ”€â”€ requirements.txt               # ZaleÅ¼noÅ›ci
â”œâ”€â”€ .env                          # Konfiguracja (nie commitowaÄ‡!)
â””â”€â”€ output_files/
    â”œâ”€â”€ lodz_restaurants_cafes_emb_input.jsonl
    â””â”€â”€ lodz_restaurants_cafes_embeddings_mean.jsonl
```

## ğŸ® UÅ¼ycie

### Podstawowy interfejs czatu
```bash
python chat_interface.py
```

PrzykÅ‚adowa konwersacja:
```
ğŸ‘¤ Ty: CzeÅ›Ä‡!
ğŸ¤– Asystent: Witaj! Jak mogÄ™ Ci pomÃ³c w znalezieniu restauracji w Åodzi?

ğŸ‘¤ Ty: Szukam dobrej pizzerii
ğŸ¤– Asystent: Åšwietnie! ZnalazÅ‚em kilka doskonaÅ‚ych pizzerii:
1. Pizzeria Napoletana - ul. Piotrkowska 50
   â­ 4.8/5.0 (1234 opinii)
   ...
```

### Komendy specjalne
- `exit`, `quit`, `q` - zakoÅ„cz program
- `clear`, `reset` - wyczyÅ›Ä‡ historiÄ™ konwersacji
- `save` - zapisz konwersacjÄ™ do JSON

### Testy systemowe
```bash
python test_system.py
```

## ğŸ”§ Konfiguracja

### conversational_rag.py

GÅ‚Ã³wne parametry do dostosowania:

```python
rag = ConversationalRAG(
    llm_client=llm,
    search_function=search,
    max_history=10,  # Liczba par w historii
)
```

W `generate_response()`:
```python
response = rag.generate_response(
    user_message,
    k=5  # Liczba wynikÃ³w wyszukiwania
)
```

### Prompt systemowy

MoÅ¼esz dostosowaÄ‡ prompt w `conversational_rag.py`:
```python
custom_prompt = """
TwÃ³j wÅ‚asny prompt systemowy...
"""

rag = ConversationalRAG(
    llm_client=llm,
    search_function=search,
    system_prompt=custom_prompt
)
```

## ğŸ§  Jak to dziaÅ‚a?

### 1. Ekstakcja zapytania
```
UÅ¼ytkownik: "Szukam dobrej pizzerii"
    â†“
LLM ekstrahuje: "pizzeria"
```

### 2. Wyszukiwanie semantyczne
```
"pizzeria" â†’ embedding (1024 wymiarÃ³w)
    â†“
FAISS wyszukuje podobne wektory
    â†“
Top 5 najbardziej podobnych restauracji
```

### 3. Generowanie odpowiedzi
```
Historia + Zapytanie + Wyniki â†’ PLLuM
    â†“
Naturalna odpowiedÅº w jÄ™zyku polskim
```

## ğŸ“Š Model embeddingÃ³w

UÅ¼ywamy **sdadas/mmlw-retrieval-roberta-large**:
- Polski model RoBERTa
- 1024 wymiarÃ³w
- Mean lub CLS pooling
- Znormalizowane embeddingi

## ğŸ” Wyszukiwanie

FAISS IndexFlatIP:
- Inner Product similarity
- DokÅ‚adne wyniki (nie przybliÅ¼one)
- Szybkie dla maÅ‚ych zbiorÃ³w (<100k)

## ğŸ¯ PrzykÅ‚ady uÅ¼ycia

### Prosty kod
```python
from conversational_rag import create_rag_system

# Inicjalizacja
rag, search = create_rag_system()

# Konwersacja
response = rag.generate_response("Szukam sushi")
print(response)

# Kolejne pytanie (pamiÄ™ta kontekst)
response = rag.generate_response("Ale w centrum miasta")
print(response)
```

### Z wÅ‚asnÄ… funkcjÄ… wyszukiwania
```python
from conversational_rag import PLLuMLLM, ConversationalRAG

llm = PLLuMLLM()

def my_search(query, k=5):
    # Twoja wÅ‚asna logika
    return results

rag = ConversationalRAG(llm, my_search)
```

## ğŸ› Troubleshooting

### BÅ‚Ä…d: "Brak HF_TOKEN"
- SprawdÅº czy plik `.env` istnieje
- SprawdÅº czy `HF_TOKEN=...` jest poprawny
- SprÃ³buj: `export HF_TOKEN='twÃ³j_token'`

### BÅ‚Ä…d: "FileNotFoundError: embeddings"
- Uruchom najpierw: `python create_embeddings_mean.py`
- SprawdÅº czy plik istnieje w `output_files/`

### Wolne odpowiedzi
- Model PLLuM-12B jest duÅ¼y (12B parametrÃ³w)
- Pierwsza odpowiedÅº jest najwolniejsza (Å‚adowanie modelu)
- RozwaÅ¼ uÅ¼ycie mniejszego modelu lub GPU

### BÅ‚Ä…d FAISS
- Dla CPU: `pip install faiss-cpu`
- Dla GPU: `pip install faiss-gpu`

## ğŸ“ TODO / Rozszerzenia

- [ ] Wsparcie dla filtrÃ³w (cena, kuchnia, ocena)
- [ ] Geolokalizacja uÅ¼ytkownika
- [ ] Wykresy i wizualizacje
- [ ] Web interface (Streamlit/Gradio)
- [ ] WielojÄ™zycznoÅ›Ä‡
- [ ] Integracja z API restauracji
- [ ] Rezerwacje stolikÃ³w
- [ ] Historia preferencji uÅ¼ytkownika (persistent)

## ğŸ“„ Licencja

MIT

## ğŸ¤ Kontakt

Pytania? Sugestie? OtwÃ³rz issue na GitHubie!

---

**Stworzone z â¤ï¸ wykorzystujÄ…c PLLuM, FAISS i RoBERTa**